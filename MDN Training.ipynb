{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Density Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import h5py\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import genconfig\n",
    "from train_MDN import train_nn, mixture_density_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import yaml\n",
    "Option = namedtuple(\"MyStruct\", \"training_input training_output validation_fraction outFile network_type \\\n",
    "                    config structure learning_rate regularizer epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Particle Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Config file, options are : 'number','pos1','pos2','pos3', 'error1x', 'error1y', 'error2x', 'error2y', 'error3x', 'error3y'  \n",
    "Below is example of pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python genconfig.py --type pos1 > pos1config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with generated config run train_nn which stores training history in text file in training_output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loading config from:  pos1config.json\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'inputs': ['NN_matrix0',\n  'NN_matrix1',\n  'NN_matrix2',\n  'NN_matrix3',\n  'NN_matrix4',\n  'NN_matrix5',\n  'NN_matrix6',\n  'NN_matrix7',\n  'NN_matrix8',\n  'NN_matrix9',\n  'NN_matrix10',\n  'NN_matrix11',\n  'NN_matrix12',\n  'NN_matrix13',\n  'NN_matrix14',\n  'NN_matrix15',\n  'NN_matrix16',\n  'NN_matrix17',\n  'NN_matrix18',\n  'NN_matrix19',\n  'NN_matrix20',\n  'NN_matrix21',\n  'NN_matrix22',\n  'NN_matrix23',\n  'NN_matrix24',\n  'NN_matrix25',\n  'NN_matrix26',\n  'NN_matrix27',\n  'NN_matrix28',\n  'NN_matrix29',\n  'NN_matrix30',\n  'NN_matrix31',\n  'NN_matrix32',\n  'NN_matrix33',\n  'NN_matrix34',\n  'NN_matrix35',\n  'NN_matrix36',\n  'NN_matrix37',\n  'NN_matrix38',\n  'NN_matrix39',\n  'NN_matrix40',\n  'NN_matrix41',\n  'NN_matrix42',\n  'NN_matrix43',\n  'NN_matrix44',\n  'NN_matrix45',\n  'NN_matrix46',\n  'NN_matrix47',\n  'NN_matrix48',\n  'NN_pitches0',\n  'NN_pitches1',\n  'NN_pitches2',\n  'NN_pitches3',\n  'NN_pitches4',\n  'NN_pitches5',\n  'NN_pitches6',\n  'NN_layer',\n  'NN_barrelEC',\n  'NN_phi',\n  'NN_theta'],\n 'targets': ['NN_position_id_X_0', 'NN_position_id_Y_0'],\n 'metadata': ['RunNumber',\n  'EventNumber',\n  'ClusterNumber',\n  'NN_sizeX',\n  'NN_sizeY',\n  'NN_localEtaPixelIndexWeightedPosition',\n  'NN_localPhiPixelIndexWeightedPosition',\n  'NN_layer',\n  'NN_barrelEC',\n  'NN_etaModule',\n  'NN_phi',\n  'NN_theta',\n  'globalX',\n  'globalY',\n  'globalZ',\n  'globalEta',\n  'NN_pitches0',\n  'NN_pitches1',\n  'NN_pitches2',\n  'NN_pitches3',\n  'NN_pitches4',\n  'NN_pitches5',\n  'NN_pitches6']}"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "options = Option(\n",
    "    training_input='NNinput_pos1_training.h5',\n",
    "    training_output='Trained',\n",
    "    validation_fraction=0.1,\n",
    "    outFile='pos1',\n",
    "    network_type='1particle',\n",
    "    config='pos1config.json',\n",
    "    structure=[100, 50, 50],\n",
    "    learning_rate=0.0001,\n",
    "    regularizer=0.0001,\n",
    "    epochs=250,\n",
    ")\n",
    "\n",
    "print('loading config from: ', options.config)\n",
    "config = open(options.config, 'r')\n",
    "yamlConfig = yaml.load(config, Loader=yaml.FullLoader)\n",
    "yamlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(['NN_matrix0', 'NN_matrix1', 'NN_matrix2', 'NN_matrix3', 'NN_matrix4', 'NN_matrix5', 'NN_matrix6', 'NN_matrix7', 'NN_matrix8', 'NN_matrix9', 'NN_matrix10', 'NN_matrix11', 'NN_matrix12', 'NN_matrix13', 'NN_matrix14', 'NN_matrix15', 'NN_matrix16', 'NN_matrix17', 'NN_matrix18', 'NN_matrix19', 'NN_matrix20', 'NN_matrix21', 'NN_matrix22', 'NN_matrix23', 'NN_matrix24', 'NN_matrix25', 'NN_matrix26', 'NN_matrix27', 'NN_matrix28', 'NN_matrix29', 'NN_matrix30', 'NN_matrix31', 'NN_matrix32', 'NN_matrix33', 'NN_matrix34', 'NN_matrix35', 'NN_matrix36', 'NN_matrix37', 'NN_matrix38', 'NN_matrix39', 'NN_matrix40', 'NN_matrix41', 'NN_matrix42', 'NN_matrix43', 'NN_matrix44', 'NN_matrix45', 'NN_matrix46', 'NN_matrix47', 'NN_matrix48', 'NN_pitches0', 'NN_pitches1', 'NN_pitches2', 'NN_pitches3', 'NN_pitches4', 'NN_pitches5', 'NN_pitches6', 'NN_layer', 'NN_barrelEC', 'NN_phi', 'NN_theta'], ['NN_position_id_X_0', 'NN_position_id_Y_0'], [])\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found: (<BatchDataset shapes: ((None, 60), (None, 2)), types: (tf.float64, tf.float64)>, None, None)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-75170bdcbced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                         \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                         \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                         \u001b[1;31m#args.momentum,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[1;31m#args.batch,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saran\\GitHub\\pixel-mdn-training-tensorflow-master\\train_MDN.py\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(training_input, training_output, validation_fraction, outFile, network_type, config, structure, learning_rate, regularizer, epochs, patience, verbose)\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0mloss_record\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         ],\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m     \u001b[0moutput_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_output\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0moutFile\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%m_%d_%H%M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\MDNtraining_TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\MDNtraining_TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    795\u001b[0m           data_adapter.train_validation_split((x, y, sample_weight),\n\u001b[0;32m    796\u001b[0m                                               \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m                                               shuffle=False))\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\MDNtraining_TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   1309\u001b[0m     raise ValueError(\n\u001b[0;32m   1310\u001b[0m         \u001b[1;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m         \"arrays, found: {}\".format(arrays))\n\u001b[0m\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found: (<BatchDataset shapes: ((None, 60), (None, 2)), types: (tf.float64, tf.float64)>, None, None)"
     ]
    }
   ],
   "source": [
    "model_2, history_2 = train_nn(\n",
    "                        options.training_input,\n",
    "                        options.training_output,\n",
    "                        options.validation_fraction,\n",
    "                        options.outFile,\n",
    "                        options.network_type,\n",
    "                        options.config,\n",
    "                        options.structure,\n",
    "                        options.learning_rate,\n",
    "                        options.regularizer,\n",
    "                        options.epochs,\n",
    "                        #args.momentum,\n",
    "                        #args.batch,\n",
    "                        ##args.min_epochs,\n",
    "                        #args.max_epochs,\n",
    "                        ##args.patience_increase,\n",
    "                        ##args.threshold,\n",
    "                        #args.min_delta,\n",
    "                        #args.patience,\n",
    "                        #args.profile,\n",
    "                        #args.nbworkers,\n",
    "                        #args.save_all,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = 'Trained'\n",
    "outFile = 'pos2'\n",
    "output_location = training_output+'/'+outFile+'_'+datetime.datetime.now().strftime('%m_%d_%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2.save(output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(output_location, custom_objects={'abs' : tf.keras.backend.abs,\n",
    "                                                                        'loss' : mixture_density_loss(nb_components=1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.x features method model.save() which creates model file that stores both the weights and architecture. Loading the model from this format requires only custom objects like the above activation and loss functions to be provided. That means it includes the config, optimizer, and weights of the model. Older Tensorflow/keras model saving format required saving a weights .h5 file as well as the config and rebuilding the model and its architecture from that, which still requires the custom objects as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve Two Particle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = np.loadtxt('Trained/pos2_06_04_0556_training_pos2.txt')\n",
    "validation_loss = np.loadtxt('Trained/pos2_06_04_0556_validation_pos2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_loss, val_loss, fileName, outputDir):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    \n",
    "    plt.plot(train_loss, color='b', linewidth=0.5)\n",
    "    plt.plot(val_loss, color='r', linewidth=0.5)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim(0,250)\n",
    "    plt.ylim(0, 0.9)\n",
    "    plt.legend(['training set','validation set'])\n",
    "    plt.savefig(outputDir+'/'+fileName+'_Loss.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(training_loss, validation_loss, 'pos2', 'Trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Particle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python genconfig.py --type pos1 > pos1config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = Option(\n",
    "    training_input='NNinput_pos1_training.h5',\n",
    "    training_output='Trained',\n",
    "    validation_fraction=0.1,\n",
    "    outFile='pos1',\n",
    "    network_type='1particle',\n",
    "    config='pos1config.json',\n",
    "    structure=[100, 80, 50],\n",
    "    learning_rate=0.0001,\n",
    "    regularizer=0.0001,\n",
    "    epochs=250,\n",
    ")\n",
    "\n",
    "print('loading config from: ', options.config)\n",
    "config = open(options.config, 'r')\n",
    "yamlConfig = yaml.load(config, Loader=yaml.FullLoader)\n",
    "yamlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1, history_1 = train_nn(\n",
    "                        options.training_input,\n",
    "                        options.training_output,\n",
    "                        options.validation_fraction,\n",
    "                        options.outFile,\n",
    "                        options.network_type,\n",
    "                        options.config,\n",
    "                        options.structure,\n",
    "                        options.learning_rate,\n",
    "                        options.regularizer,\n",
    "                        options.epochs,\n",
    "                        #args.momentum,\n",
    "                        #args.batch,\n",
    "                        ##args.min_epochs,\n",
    "                        #args.max_epochs,\n",
    "                        ##args.patience_increase,\n",
    "                        ##args.threshold,\n",
    "                        #args.min_delta,\n",
    "                        #args.patience,\n",
    "                        #args.profile,\n",
    "                        #args.nbworkers,\n",
    "                        #args.save_all,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Particle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python genconfig.py --type pos3 > pos3config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = Option(\n",
    "    training_input='NNinput_pos3_training.h5',\n",
    "    training_output='Trained',\n",
    "    validation_fraction=0.1,\n",
    "    outFile='pos3',\n",
    "    network_type='3particle',\n",
    "    config='pos3config.json',\n",
    "    structure=[100, 80, 50],\n",
    "    learning_rate=0.0001,\n",
    "    regularizer=0.0001,\n",
    "    epochs=250,\n",
    ")\n",
    "\n",
    "print('loading config from: ', options.config)\n",
    "config = open(options.config, 'r')\n",
    "yamlConfig = yaml.load(config, Loader=yaml.FullLoader)\n",
    "yamlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_3, history_3 = train_nn(\n",
    "                        options.training_input,\n",
    "                        options.training_output,\n",
    "                        options.validation_fraction,\n",
    "                        options.outFile,\n",
    "                        options.network_type,\n",
    "                        options.config,\n",
    "                        options.structure,\n",
    "                        options.learning_rate,\n",
    "                        options.regularizer,\n",
    "                        options.epochs,\n",
    "                        #args.momentum,\n",
    "                        #args.batch,\n",
    "                        ##args.min_epochs,\n",
    "                        #args.max_epochs,\n",
    "                        ##args.patience_increase,\n",
    "                        ##args.threshold,\n",
    "                        #args.min_delta,\n",
    "                        #args.patience,\n",
    "                        #args.profile,\n",
    "                        #args.nbworkers,\n",
    "                        #args.save_all,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p = tf.keras.models.load_model('Trained/pos2_06_04_0556', custom_objects={'abs' : tf.keras.backend.abs,\n",
    "                                                                        'loss' : mixture_density_loss(nb_components=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('NNinput_pos2_training.h5', 'r') as hf:  \n",
    "    #x_train = hf['train_data_x'][:]\n",
    "    #y_train = hf['train_data_y'][:]\n",
    "    x_valid = hf['valid_data_x'][:]\n",
    "    y_valid = hf['valid_data_y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "branches = utils.get_data_config_names('pos2config.json', meta=False)\n",
    "print(branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_2p.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = np.hstack((y_pred[0][:,1:3], y_pred[1][:,1:3]))\n",
    "pred_mean[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prec = np.hstack((y_pred[0][:,3:5], y_pred[1][:,3:5]))\n",
    "pred_prec[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(y_pred[0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_1 = tf.reshape(x_valid[0], [1,60])\n",
    "x_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_1 = model_2p.predict(x_test_1)\n",
    "y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('MDNtraining_TF': conda)",
   "language": "python",
   "name": "python361064bitmdntrainingtfconda41395b29b3f64ff7b5d12b9937ed94c7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}